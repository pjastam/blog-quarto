{
  "hash": "93b649f3999b0dd1999ae9f36a0feddc",
  "result": {
    "markdown": "---\ntitle: \"Use tidymodels with weighted and unweighted data\"\ndescription: \"It took some time to figure out how to use frequency weights when fitting a model in the tidymodels framework. Here is the code to do that.\"\nauthor: \"Piet Stam\"\ndate: \"2022-09-19\"\ncategories: [digital transformation]\nimage: \"luis-reyes-mTorQ9gFfOg-unsplash.png\"\nimage-alt: 'Photo by <a href=\"https://unsplash.com/@tuga760?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Luis Reyes</a> on <a href=\"https://unsplash.com/s/photos/weights?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>'\nreference-location: document\ndraft: true\neditor: visual\n---\n\n\n## Use case\n\nThe [tidymodels](https://www.tidymodels.org/) framework is a collection of packages for modeling and machine learning using tidyverse principles. The [get started](https://www.tidymodels.org/start/) case study helps to take the first steps. Another helpful source is [lesson 10](https://www.gmudatamining.com/lesson-10-r-tutorial.html) of an R tutorial from a [data mining course](https://www.gmudatamining.com/index.html) at George Mason University.\n\nBuilding on these basics, my next step is to apply frequency weights when estimating a linear regression model in the tidymodels way of coding. However, [this blog post](https://www.tidyverse.org/blog/2022/05/case-weights/) shows that this is a feature [under development](https://github.com/tidymodels/planning/tree/main/case-weights) and therefore some of my first attempts to create a reproducible example failed.\n\nThe tidymodels [how-to add case weights to a workflow](https://workflows.tidymodels.org/reference/add_case_weights.html?q=add_case#ref-examples) gives some examples with code that helps to crack the case. Below I give the code for two reproducible examples, one example of model estimation without using weights and one with using weights.\n\n## Data and method\n\nThe models that I estimate are linear regression models with a set of predictors and one numeric outcome variable. The parameters of this model are estimated by ordinary least squares.\n\nI use the `car_prices` data set for the examples and try to predict the car prices with the care brands as predictors. Note that, as a consequence, in my examples the outcome variable is non-negative and the predictors are mutually exclusive (0/1) dummy variables. This makes the examples easy to understand, but the code may apply to a wider range of variables nonetheless. I use `mileage` as the weighting variable.\n\nLet us start with loading the data into memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load library for the recipe. parsnip, workflow and hardhat packages, along with the rest of tidymodels\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.1      ✔ recipes      1.0.1 \n✔ dials        1.0.0      ✔ rsample      1.1.0 \n✔ dplyr        1.0.10     ✔ tibble       3.1.8 \n✔ ggplot2      3.3.6      ✔ tidyr        1.2.0 \n✔ infer        1.0.3      ✔ tune         1.0.0 \n✔ modeldata    1.0.0      ✔ workflows    1.0.0 \n✔ parsnip      1.0.1      ✔ workflowsets 1.0.0 \n✔ purrr        0.3.4      ✔ yardstick    1.0.0 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n:::\n:::\n\n\nNow we select only the relevant variables. Although the weights are not yet used in the first example, `mileage` is already defined as the weighting variable in the data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data set with one non-negative continuous variable and uncorrelated dummy variables as predictors\ndb <- select(car_prices, Price, Buick:Saturn, Mileage) %>% \n  mutate(Mileage = frequency_weights(Mileage))\nstr(db)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [804 × 8] (S3: tbl_df/tbl/data.frame)\n $ Price   : num [1:804] 22661 21725 29143 30732 33359 ...\n $ Buick   : int [1:804] 1 0 0 0 0 0 0 0 0 0 ...\n $ Cadillac: int [1:804] 0 0 0 0 0 0 0 0 0 0 ...\n $ Chevy   : int [1:804] 0 1 0 0 0 0 0 0 0 0 ...\n $ Pontiac : int [1:804] 0 0 0 0 0 0 0 0 0 0 ...\n $ Saab    : int [1:804] 0 0 1 1 1 1 1 1 1 1 ...\n $ Saturn  : int [1:804] 0 0 0 0 0 0 0 0 0 0 ...\n $ Mileage : freq_wts [1:804] 20105, 13457, 31655, 22479, 17590, 23635, 17381, 2755...\n```\n:::\n:::\n\n\n## Example 1: linear regression without weights\n\nNow on with the first example. In the code below we define the `recipe`, define the model and set mode and engine. These are combined into a workflow. Afterwards we look at the properties of these objects to check if these are as expected. Note that `Saturn` is the reference dummy variable of my choice (i.e. in effect its coefficient is set to zero by default) and is thus excluded from the regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get data ready for modeling with recipe package\nrecipe1 <-\n  db %>% \n  recipe(Price ~ 1 + Buick + Cadillac + Chevy + Pontiac + Saab) # add all dummy variables but one\n\n# Define model, mode and engine with parsnip package\nmodel1 <-\n  linear_reg() %>% # adds the basic model type\n  set_engine('lm') %>% # adds the computational engine to estimate the model parameters\n  set_mode('regression') # adds the modeling context in which it will be used\n\n# Bundle pre-processing, modeling, and post-processing with workflow package\nworkflow1 <-\n  workflow() %>%\n  add_recipe(recipe1) %>%\n  add_model(model1)\n\n# View object properties\nrecipe1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          5\n```\n:::\n\n```{.r .cell-code}\nmodel1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n\n```{.r .cell-code}\nworkflow1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\nNow that the objects look alright, the model estimation can be performed and the parameter estimates are printed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now estimate the model via a single call to fit()\nfit1 <- fit(workflow1, data = db)\n\n# View fit1 properties\ntidy(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   13979.      763.     18.3  7.62e- 63\n2 Buick          6836.     1009.      6.77 2.46e- 11\n3 Cadillac      26958.     1009.     26.7  9.16e-113\n4 Chevy          2449.      832.      2.94 3.32e-  3\n5 Pontiac        4433.      903.      4.91 1.10e-  6\n6 Saab          15516.      943.     16.5  1.26e- 52\n```\n:::\n\n```{.r .cell-code}\nglance(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.s…¹ sigma stati…²   p.value    df logLik    AIC    BIC devia…³\n      <dbl>     <dbl> <dbl>   <dbl>     <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.645     0.642 5911.    290. 1.53e-176     5 -8120. 16254. 16287. 2.79e10\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\n\n## Example 2: linear regression with weights\n\nThen come the weights. The first thought is to update the current workflow with a line of code to make clear that weights should be used. However, this approach does not produce the desired result.\n\nTherefore, an alternative approach is followed. Instead of building upon the blocks of the first example, we start with a new `workflow()` object and add an `add_case_weights` line of code to it. Next, one would expect a line of code with an `add_recipe` command, but for some reason this did not work after a \"few\" tries. Instead, we use `add_formula` with the regression formula as an argument. Lastly, surprisingly conventional, an `add_model` command is added.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nworkflow2 <-\n  workflow() %>%\n  add_case_weights(Mileage) %>%\n  add_formula(Price ~ 1 + Buick + Cadillac + Chevy + Pontiac + Saab) %>%\n  add_model(model1)\n\nworkflow2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nPrice ~ 1 + Buick + Cadillac + Chevy + Pontiac + Saab\n\n── Case Weights ────────────────────────────────────────────────────────────────\nMileage\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\nNow the parameters are estimated with one line of code as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- fit(workflow2, db)\n\n# View fit2 properties\ntidy(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   13448.      694.     19.4  8.70e- 69\n2 Buick          7006.      918.      7.63 6.52e- 14\n3 Cadillac      26152.      933.     28.0  7.96e-121\n4 Chevy          2452.      759.      3.23 1.28e-  3\n5 Pontiac        4647.      828.      5.61 2.73e-  8\n6 Saab          15349.      853.     18.0  5.72e- 61\n```\n:::\n\n```{.r .cell-code}\nglance(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.…¹  sigma stati…²   p.value    df logLik    AIC    BIC devia…³\n      <dbl>    <dbl>  <dbl>   <dbl>     <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.664    0.661 7.67e5    315. 5.90e-186     5 -8111. 16237. 16269. 4.70e14\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n:::\n\n\nThis is a nice first try! With the two examples above it is possible to experiment further in the hope of alternative/shorter routes to the estimation results. In the mean time, we wait for the tidymodels to include weights in the relevant packages. If you are inspired by these two examples (or not) and have some new ideas for progress, do not hesitate to [give feedback to the Tidyverse developers](https://www.tidyverse.org/blog/2022/05/case-weights/#getting-feedback).\n\nHappy coding!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}