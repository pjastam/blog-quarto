{
  "hash": "f5b19dceab680ede072f52426d71237a",
  "result": {
    "markdown": "---\ntitle: \"Regression Analysis of Aggregate Data in R\"\ndescription: \"Aggregate microdata with a continuous outcome variable and categorical predictors and derive the individual-level regression coefficients and standard errors afterwards.\"\nauthor: \"Piet Stam\"\ndate: \"2023-02-26\"\ndate-modified: \"2023-03-05\"\ncategories: [R]\nimage: \"640px-Diagram_of_aggregate_data_chrom.png\"\nimage-alt: \"Image by [Chubbylilrabbit - Own work](https://commons.wikimedia.org/w/index.php?curid=96172095), CC BY-SA 4.0, adapted by Piet Stam\"\nreference-location: margin\ndraft: false\neditor: visual\nexecute: \n  warning: false\n  error: true\n---\n\n\n## Introduction\n\nWhen dealing with large sets of data or sensitive individual-level information, researchers often use a technique to vertically [aggregate the data](https://en.wikipedia.org/wiki/Aggregate_data). This involves combining the data into larger groups to conserve computer resources or maintain privacy.\n\nBut a key question arises: can statistical analyses based on this aggregated data still yield accurate results? The answer, at least for regression analysis, is a resounding yes. In fact, we can use the estimation results based on the aggregate data to derive the estimates of a linear regression based on the original, individual-level data.\n\nTo help illustrate this concept, I've written R code to reproduce an example from a scientific publication where the authors provided the data and code in SAS. With this example, you can see how to aggregate the data and derive the original regression results.\n\n## The example\n\nRahim Moineddin and Marcelo Luis Urquia have shown that the estimated coefficients of an individual-level regression model and aggregate-level regression model are identical in case of a continuous outcome variable and categorical predictors.[^1] Although such equality does not hold for the standard errors of these point estimates, however, they show how to derive the individual-level standard errors from the aggregate data. Furthermore, they added SAS code and the example data set to show us the way.[^2] In this post, I provide the R code to reproduce this example.\n\n[^1]: They are the authors of the publication \"[Regression analysis of aggregate continuous data](https://doi.org/10.1097/ede.0000000000000172)\".\n\n[^2]: These can be found in their [Supplemental Digital Content](https://links.lww.com/EDE/A830) document.\n\n## Read data\n\nThe example data that I will use is in the file called `wic.txt`. These are the example data that the aforementioned authors use to illustrate the application of their method in SAS. In this example, they focused on the association between receipt of [WIC](http://www.fns.usda.gov/wic/about-wic) (The Special Supplemental Nutrition Program for Women, Infants, and Children) food for the mother during this pregnancy and gestational weight gain. The example data are scraped from the fourth section of their Supplemental Digital Content document.\n\nIt contains four variables:\n\n-   Weight gain during pregnancy in pounds (lb) (`wtgain`)\n\n-   Receipt of WIC food (`wic`)\n\n-   Race/ethnicity (`mracethn`)\n\n-   Late or no prenatal care (`latecare`)\n\nLet's read the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readtext)\nlibrary(dplyr)\ndata <- readtext(\"https://cdn-links.lww.com/permalink/ede/a/ede_25_6_2014_07_23_urquia_ede14-408_sdc1.doc\") %>% \n  subset(select = \"text\") %>% \n  { read.table(text=sub(\".*4) individual-level dataset\", \"\", .), header=TRUE) }\n```\n:::\n\n\n## Apply transformations\n\nBefore we perform the regression, we apply data transformations in the same way that they are applied in the second section of the Supplemental Digital Content document. First, the weight gain during pregnancy in kilograms (`wtgaink`) is calculated by multiplication of the weight gain during pregnancy in pounds (lb) (`wtgain`) by the conversion factor `0.453592`. Second, the categorical variables `wic`, `mracethn` and `latecare` are transformed to the factor type. Third, the reference level of the categorical variables `mracethn` is set to the last category called 'Non-Hispanic Blacks'. Finally, the `wtgain` variable is no longer needed and therefore dropped.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndb_indiv <- data %>%\n  rename_all(tolower) %>%\n  mutate(\n    wtgaink = wtgain * 0.453592,\n    wic = as.factor(wic),\n    mracethn = as.factor(mracethn),\n    mracethn = relevel(mracethn, ref = 3),\n    latecare = as.factor(latecare)\n  ) %>%\n  select(wtgaink, wic, mracethn, latecare)\n```\n:::\n\n\n## Fit regression model\n\nThe table presented in the publication by Moineddin and Urquia shows the estimation results of three models. The first model has a single predictor, the second model three predictors and the third model includes interaction effects among some of the aforementioned predictors. Note that in all three models, the continuous outcome `wtgaink` is predicted by a set of variables which are exclusively of the categorical type.\n\nFor reasons of brevity, let's do the exercise for one of the three models. We choose the second model as this is also the model that is coded in SAS in the aforementioned Supplemental Digital Content document. Let's first estimate the model parameters based on individual data. The results appear to be the same as to those in the aforementioned table, with one notable exception: the estimates of the constant term.[^3]\n\n[^3]: I did a quick check and got the authors' estimate of the constant term after some recoding of the predictor variables. You can do this quick check yourself with this [gist](https://gist.github.com/pjastam/14cfd2a60b0787239ed6a2f18c3ec03b#file-quick-check-r).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_indiv <-\n  lm(\n    formula = wtgaink ~ wic + mracethn + latecare, \n    data = db_indiv\n  )\nsumm_indiv <- summary(model_indiv)\nsumm_indiv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = wtgaink ~ wic + mracethn + latecare, data = db_indiv)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.2466  -4.1155  -0.6376   3.4447  28.6329 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  14.5874     0.2620  55.680  < 2e-16 ***\nwicY          0.5652     0.2073   2.726  0.00642 ** \nmracethn1    -0.4928     0.2443  -2.017  0.04374 *  \nmracethn2     1.0940     0.2232   4.902 9.76e-07 ***\nlatecare1    -0.2014     0.1670  -1.206  0.22773    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.942 on 5265 degrees of freedom\nMultiple R-squared:  0.01552,\tAdjusted R-squared:  0.01477 \nF-statistic: 20.75 on 4 and 5265 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Aggregate data\n\nNext let's aggregate the data set. The aggregate data set contains averages of the outcome variable and frequencies of the categorical variables *for all combinations of the set of categorical predictors*. Thus, for all unique combinations of the categorical variables, we calculate the number of individual records `N`, the mean `meany` of the continuous variable `wtgaink`, and its standard deviation `stdy`. These aggregates suffice for our exercise to derive the individual-level point estimates and their standard errors afterwards.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndb_aggr <-\n  db_indiv %>%\n  group_by(across(c(wic, mracethn, latecare))) %>%\n  summarise(\n    meany = mean(wtgaink),\n    stdy = sd(wtgaink),\n    N = length(wtgaink)\n  ) %>%\n  ungroup()\n```\n:::\n\n\n## Derive original estimates\n\nNow it's time to derive the individual-level estimates of the coefficients and standard errors based on these aggregate results. Based on the aggregate data, a weighted regression model for the `meany` variable is fitted with `N` being the weight variable. Note that the variable `stdy` is not used in this step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_aggr <-\n  lm(\n    formula = meany ~ wic + mracethn + latecare,\n    data = db_aggr,\n    weights = N\n  )\nsumm_aggr <- summary(model_aggr)\nsumm_aggr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = meany ~ wic + mracethn + latecare, data = db_aggr, \n    weights = N)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-7.2204 -3.7597 -0.2019  4.1956  7.4323 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  14.5874     0.2682  54.380 1.86e-10 ***\nwicY          0.5652     0.2122   2.663  0.03234 *  \nmracethn1    -0.4928     0.2502  -1.970  0.08949 .  \nmracethn2     1.0940     0.2285   4.788  0.00199 ** \nlatecare1    -0.2014     0.1710  -1.178  0.27721    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.084 on 7 degrees of freedom\nMultiple R-squared:  0.9188,\tAdjusted R-squared:  0.8723 \nF-statistic: 19.79 on 4 and 7 DF,  p-value: 0.0006444\n```\n:::\n:::\n\n\nDeriving the coefficients appears to be very easy, because their estimates at the aggregate level are identical to those at the individual level and therefore do not need any adjustment. A quick check confirms this equality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.equal(coef(model_indiv),coef(model_aggr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nIn order to derive the standard errors, however, a number of data transformations are necessary. To understand the purpose of these transformations, notice that there exists a fixed ratio between the individual level standard errors and those at the aggregate level.[^4] With our data this fixed ratio is equal to 0.9766648 for all estimated coefficients:\n\n[^4]: This observation was also done [here](https://stats.stackexchange.com/questions/83223/standard-errors-in-weighted-least-squares-on-aggregated-data), using example data with both categorical outcome and predictor variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsumm_indiv$coef[,2]/summ_aggr$coef[,2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)        wicY   mracethn1   mracethn2   latecare1 \n  0.9766648   0.9766648   0.9766648   0.9766648   0.9766648 \n```\n:::\n:::\n\n\nThus, given the aggregate data and estimations results, it seems a logical strategy to first calculate this fixed ratio and then mutiply the aggregate-level standard errors by its inverse in order to arrive at the individual-level standard errors. This inverse is called an inflation factor. The original SAS code for this procedure can be found in the Supplemental Digital Content document.\n\nMy R code for deriving the individual-level standard errors is given below. For clarity, I added a short description of what's being calculated above each line of code. We estimate the inflation factor by the ratio of the square roots of the aggregate-level residual variance `errorms` and the so-called pooled variance `p_v`.[^5] We call this ratio `factor`. It is an estimate of the inflation factor, because the pooled variance in the denominator is an estimate of the variance of the individual-level outcome variable. Finally, the aggregate-level standard errors called `StdErr` are multiplied by this `factor` to derive the individual-level standard errors called `standard_error` that we are looking for. \n\n[^5]: See [Wikipedia](https://en.wikipedia.org/wiki/Pooled_variance#Definition_and_computation) for a definition of pooled variance (assuming non-uniform sample sizes).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandard_errors <-\n  db_aggr %>%\n  mutate(\n    # degrees of freedom used to calculate stdy\n    n_1 = N - 1,\n    # total sum of squares derived from stdy\n    S_n_1 = stdy ^ 2 * n_1\n  ) %>%\n  summarise(\n    # degrees of freedom\n    n_k = sum(n_1),\n    # total sum of squares\n    S2_k = sum(S_n_1),\n    # pooled variance (i.e. weighted average of group variances)\n    p_v = S2_k / n_k,\n    # aggregate-level residual variance\n    errorms = summ_aggr$sigma ^ 2,\n    # inflation factor\n    factor = sqrt(p_v / errorms),\n    # aggregate-level standard errors\n    StdErr = summ_aggr$coef[ , 2],\n    # individual-level standard errors\n    standard_error = StdErr * factor\n  )\n```\n:::\n\n\n## Show table\n\nThe variable `factor` has 5 identical elements equal to 0.9766333, which differs only very slightly from the true inflation factor. The resulting standard errors are reported in the table below. These match the standard errors estimated by the individual-level regression as desired. In addition to the standard errors, the table also shows the estimated coefficients, the lower and upper bounds of the confidence intervals, the z-scores and the p-values. As a validity check, compare the estimates reported here with the individual-level regression estimates shown above and with the estimates reported in the Moineddin and Urquia publication.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(insight)\nstandard_errors %>%\n  mutate(\n    estimate = coefficients(model_aggr),\n    z = estimate / standard_error,\n    p = 2 * (1 - pnorm(abs(z))),\n    pvalue = format_p(p, stars = TRUE),\n    CI_low = estimate - 1.96 * standard_error,\n    CI_high = estimate + 1.96 * standard_error,\n  ) %>%\n  bind_cols(\"predictors\" = names(standard_errors$standard_error)) %>% \n  select(predictors, estimate, standard_error, CI_low, CI_high, z, pvalue) %>%\n  format_table(digits = 3, ci_digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   predictors estimate standard_error               CI      z      pvalue\n1 (Intercept)   14.587          0.262 [14.074, 15.101] 55.681 p < .001***\n2        wicY    0.565          0.207 [ 0.159,  0.971]  2.726 p = 0.006**\n3   mracethn1   -0.493          0.244 [-0.972, -0.014] -2.017  p = 0.044*\n4   mracethn2    1.094          0.223 [ 0.657,  1.531]  4.902 p < .001***\n5   latecare1   -0.201          0.167 [-0.529,  0.126] -1.206   p = 0.228\n```\n:::\n:::\n\n\n## What's next?\n\nIn the above example the ordinary least-squares algorithm was used to estimate the parameters of a linear regression. With the R code presented here, one can apply this algorithm to aggregate data and derive both point estimates and standard errors afterwards to reduce the use of scarce computer resources.[^6] As this procedure is especially useful for doing linear regression based on BIG data, a logical next question is whether vertical aggregation is also helpful with other algorithms usually applied to BIG data as well, such as random forest. This may be the subject of another blog post.\n\n[^6]: Here is a [gist](https://gist.github.com/pjastam/14cfd2a60b0787239ed6a2f18c3ec03b#file-ols-estimates-aggegrate-data-r) with the complete example code.\n\nHappy coding!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}